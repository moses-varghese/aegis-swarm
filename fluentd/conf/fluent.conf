# # This is the input source. 'forward' is the protocol Docker's logging driver uses.
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# NEW: This filter block parses the nested JSON log from Docker
<filter docker.**>
  @type parser
  key_name log
  reserve_data true
  remove_key_name_field true
  <parse>
    # Use the multi_format parser
    @type multi_format
    # The <pattern> blocks are tried in order.
    <pattern>
      # First, try to parse the log as JSON.
      format json
    </pattern>
    <pattern>
      # If JSON fails, parse it as plain text using a regex.
      # This regex captures the entire line into a 'message' field.
      format /^(?<message>.*)/
    </pattern>
    # @type json
  </parse>
</filter>

# # This block matches all incoming logs (docker.**)
<match docker.**>
  @type copy
  <store>
    # @type elasticsearch
    @type elasticsearch_data_stream 
    host elasticsearch # The service name from docker-compose.yml
    port 9200
    # scheme http
    logstash_format true
    logstash_prefix fluentd
    logstash_dateformat %Y%m%d
    include_tag_key true
    # type_name _doc
    type_name app_log
    tag_key @log_name
    flush_interval 1s
    # Naming convention for the data stream
#     # --- CONTROL THE NAMING PATTERN HERE ---
#     # 1. The type of data (e.g., logs, metrics)
#     data_stream_type "logs"
    
#     # 2. The dataset or service group
#     data_stream_dataset "aegis-swarm"
#     # 3. A custom namespace for this specific stream
#     # data_stream_namespace "production" 
#     # --- END OF NAMING CONTROL ---
    data_stream_name "fluentd"
  </store>
  <store>
    # Also print logs to the console for easy debugging
    @type stdout
  </store>
</match>


# # This match block uses the new plugin
# <match **>
#   @type copy
#   <store>
#     # Use the new plugin type
#     # @type elasticsearch_data_stream 
#     @type elasticsearch 
    
#     # Connection settings are simpler
#     host elasticsearch
#     port 9200
    
#     # Naming convention for the data stream
#     # --- CONTROL THE NAMING PATTERN HERE ---
#     # 1. The type of data (e.g., logs, metrics)
#     data_stream_type "logs"
    
#     # 2. The dataset or service group
#     data_stream_dataset "aegis-swarm"
#     # 3. A custom namespace for this specific stream
#     # data_stream_namespace "production" 
#     # --- END OF NAMING CONTROL ---
#     data_stream_name "fluentd"
#   </store>
#   <store>
#     @type stdout
#   </store>
# </match>